{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numba\n",
    "from abc import ABC, abstractmethod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@numba.njit\n",
    "def one_hotify(t, size):\n",
    "    t_oh = np.zeros((t.shape[0], size))\n",
    "    for idx, i in enumerate(t):\n",
    "        t_oh[idx][i] = 1\n",
    "        \n",
    "    return t_oh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Network Blocks and their Derivatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@numba.njit\n",
    "def d_softmax(x):\n",
    "    return softmax(x) * (1 - softmax(x))\n",
    "\n",
    "\n",
    "@numba.njit\n",
    "def d_relu(x):\n",
    "    return (x > 0).astype(np.float32)\n",
    "\n",
    "@numba.njit\n",
    "def clip_gradients(gradients, max_norm=1.0):\n",
    "    if max_norm is None:\n",
    "        return gradients\n",
    "    \n",
    "    norm = np.linalg.norm(gradients, ord=2)\n",
    "    clip_coeff = max_norm / (norm + 1e-6)\n",
    "    if clip_coeff < max_norm:\n",
    "        clipped = gradients * clip_coeff\n",
    "    else:\n",
    "        clipped = gradients\n",
    "        \n",
    "    return clipped\n",
    "\n",
    "\n",
    "class Layer(ABC):\n",
    "    @abstractmethod\n",
    "    def back_fn(self, prev, x):\n",
    "        pass\n",
    "\n",
    "    def update_weights(self, dW, db, lr=1e-3, clip_val=1.0):\n",
    "        pass\n",
    "\n",
    "\n",
    "class LinearLayer(Layer):\n",
    "    def __init__(self, in_shape, out_shape):\n",
    "        self.in_shape = in_shape\n",
    "        self.out_shape = out_shape\n",
    "        limit = np.sqrt(6 / (in_shape + out_shape))\n",
    "        self.weights = np.random.uniform(-limit, limit, (in_shape, out_shape))\n",
    "        self.bias = np.zeros((1, out_shape))\n",
    "\n",
    "    def back_fn(self, prev, x):\n",
    "        dW = np.dot(x.T, prev)  # Assuming x and prev are 2D matrices\n",
    "\n",
    "        # Gradient of the loss with respect to the biases\n",
    "        db = np.sum(prev, axis=0, keepdims=True)  # Summing over all samples if in batch\n",
    "\n",
    "        # Gradient of the loss with respect to the input of this layer\n",
    "        dx = np.dot(prev, self.weights.T)\n",
    "\n",
    "        return dx, dW, db\n",
    "\n",
    "    def update_weights(self, dW, db, lr=1e-3, clip_val=1.0):\n",
    "        self.weights -= (clip_gradients(dW, clip_val) * lr)\n",
    "        self.bias -= (clip_gradients(db, clip_val) * lr)\n",
    "\n",
    "    def __call__(self, x):\n",
    "        return x @ self.weights + self.bias\n",
    "\n",
    "    def __str__(self):\n",
    "        return f\"LinearLayer(in_shape={self.in_shape}, out_shape={self.out_shape})\"\n",
    "\n",
    "\n",
    "class ReluLayer(Layer):\n",
    "    def back_fn(self, prev, x):\n",
    "        drelu = d_relu(x)\n",
    "        dx = prev * drelu\n",
    "        return dx, None, None\n",
    "\n",
    "    def __call__(self, x):\n",
    "        return np.where(x > 0, x, 0)\n",
    "\n",
    "    def __str__(self):\n",
    "        return f\"ReluLayer()\"\n",
    "\n",
    "\n",
    "def softmax(x):\n",
    "    # Shift x for numerical stability by subtracting its max value in each row\n",
    "    shift_x = x - np.max(x, axis=1, keepdims=True)\n",
    "    exp_shift_x = np.exp(shift_x)\n",
    "    softmax = exp_shift_x / np.sum(exp_shift_x, axis=1, keepdims=True)\n",
    "    return softmax\n",
    "\n",
    "\n",
    "class SoftmaxLayer:\n",
    "    def __call__(self, x):\n",
    "        return softmax(x)\n",
    "    \n",
    "    def __str__(self):\n",
    "        return \"SoftmaxLayer()\"\n",
    "\n",
    "\n",
    "def log_softmax(x):\n",
    "    x_off = x - np.max(x)\n",
    "    return x_off - np.log(np.sum(np.exp(x_off), axis=-1, keepdims=True))\n",
    "\n",
    "\n",
    "class ComposedNetwork:\n",
    "    def __init__(self, *layers):\n",
    "        self.layers = layers\n",
    "        self.inputs = []\n",
    "\n",
    "    def backward(self, dx, lr=1e-3, clip_val=1.0):\n",
    "        \"\"\"dx should be the gradient of the cost function.\"\"\"\n",
    "        for layer, input in zip(reversed(self.layers), reversed(self.inputs)):\n",
    "            dx, dW, db = layer.back_fn(dx, input)\n",
    "            layer.update_weights(dW, db, lr)\n",
    "            \n",
    "        self.inputs.clear()\n",
    "\n",
    "    def __call__(self, x):\n",
    "        self.inputs.clear()\n",
    "        for layer in self.layers:\n",
    "            self.inputs.append(x)\n",
    "            x = layer(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def __str__(self):\n",
    "        compose = \"ComposedNetwork(\\n\"\n",
    "        for layer in self.layers:\n",
    "            compose += \"\\t\"\n",
    "            compose += str(layer)\n",
    "            compose += \",\\n\"\n",
    "        compose += \")\"\n",
    "\n",
    "        return compose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network = ComposedNetwork(\n",
    "    *[\n",
    "        LinearLayer(768, 128),\n",
    "        ReluLayer(),\n",
    "        LinearLayer(128, 128),\n",
    "        ReluLayer(),\n",
    "        LinearLayer(128, 128),\n",
    "        ReluLayer(),\n",
    "        LinearLayer(128, 10),\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(network)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@numba.njit\n",
    "def nll_loss(pred, target):\n",
    "    s = 0\n",
    "    idx = 0\n",
    "    for t in target:\n",
    "        s += pred[idx][t]\n",
    "        idx += 1\n",
    "    \n",
    "    return -s / idx\n",
    "    \n",
    "\n",
    "def cross_entropy(y_pred, y_true):\n",
    "    # Avoid division by zero\n",
    "    y_pred = np.clip(y_pred, 1e-15, 1 - 1e-15)\n",
    "    \n",
    "    # Compute the loss for each sample and class\n",
    "    loss = -np.sum(y_true * log_softmax(y_pred)) / y_pred.shape[0]\n",
    "    return loss\n",
    "\n",
    "def d_cross_entropy(y_pred, y_true):\n",
    "    # Compute the gradient\n",
    "    grad = y_pred - y_true\n",
    "    return grad\n",
    "\n",
    "\n",
    "def mse_loss(pred, target):\n",
    "    return np.mean((target - pred) ** 2)\n",
    "\n",
    "def d_mse(pred, target):\n",
    "    n = pred.shape[0]  # Assuming pred and target are numpy arrays of the same shape\n",
    "    return (2/n) * (pred - target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dummy Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = np.random.randint(0, 63, (128,))\n",
    "inputs = one_hotify(targets, 64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Network\n",
    "\n",
    "Let's try and overfit to our little dummy dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network = ComposedNetwork(\n",
    "    *[\n",
    "        LinearLayer(64, 128),\n",
    "        ReluLayer(),\n",
    "        LinearLayer(128, 128),\n",
    "        ReluLayer(),\n",
    "        LinearLayer(128, 128),\n",
    "        ReluLayer(),\n",
    "        LinearLayer(128, 64),\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "n_iter = 10000\n",
    "lr = 1e-3\n",
    "\n",
    "loss_chart = []\n",
    "acc_chart = []\n",
    "for i in tqdm(range(n_iter)):\n",
    "    out = network(inputs)\n",
    "    \n",
    "    loss = cross_entropy(out, inputs)\n",
    "    acc = (np.argmax(softmax(out), axis=1) == np.argmax(inputs, axis=1)).mean()\n",
    "    \n",
    "    if np.isnan(loss):\n",
    "        break\n",
    "    \n",
    "    d_loss = d_cross_entropy(out, inputs)\n",
    "    \n",
    "    if i % 1000 == 999:\n",
    "        i *= 0.5\n",
    "\n",
    "    network.backward(d_loss, lr)\n",
    "    \n",
    "    loss_chart.append(loss)\n",
    "    acc_chart.append(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
